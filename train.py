# -*- coding: utf-8 -*-
"""Train.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/17SY4I5fp4wMEntkmDOGuDN01ZLTi2B7P
"""

import os
import torch
from transformers import AutoModelForCausalLM, AutoTokenizer, Trainer, TrainingArguments, Trainer, pipeline, BitsAndBytesConfig
from peft import LoraConfig
from datasets import Dataset
import datasets
from trl import SFTTrainer, PPOTrainer, SFTConfig
from format import prompt_phi, prompt_qwen, prompt_llama
from tqdm import tqdm
import transformers
from typing import Optional, Dict, Sequence
import copy
import logging
import random
from dataclasses import dataclass, field
#load model name
# model_name = "qwen/Qwen2.5-0.5B"
import argparse
from credential import HUGGINGFACE_TOKEN
argparser = argparse.ArgumentParser()
argparser.add_argument('--model_name', '-m', type=str, default='microsoft/Phi-3.5-mini-instruct')
argparser.add_argument('--dataset', '-d', type=str, default='metamath')
argparser.add_argument('--model_path', type=str, default=None)
argparser.add_argument('--type', type=str, default='phi')
# argparser.add_argument('--bf16', type=bool, default=False)
argparser.add_argument('--quant', '-q', type=bool, default=False)
#rank of lora
argparser.add_argument('--rank', '-r', type=int, default=64)
argparser.add_argument('--num_samples', '-n', type=int, default=None)
argparser.add_argument('--save_path', type=str, default=None)
argparser.add_argument('--save_strategy', '-s', type=str, default='epoch')
argparser.add_argument('--save_steps', '-ss', type=int, default=60000)
argparser.add_argument('--max_length', '-ml', type=int, default=512)
argparser.add_argument('--padding', '-p', type=str, default= 'do_not_pad' )
argparser.add_argument('--attn', '-a', type=str, default= None)
argparser.add_argument('--batch_size', '-b', type=int, default= 1)
argparser.add_argument('--push_to_hub', '-ph', type=bool, default=False)
argparser.add_argument('--filter', '-f', type=str, default=None)
args = argparser.parse_args()

model_name = args.model_name
model_path = args.model_path
type = args.type
# model_name = "bkai-foundation-models/vietnamese-llama2-7b-120GB"
compute_dtype = torch.bfloat16
attn_implementation = args.attn #"flash_attention_2"
quant_config = BitsAndBytesConfig(
    load_in_4bit=True,
    bnb_4bit_quant_type="nf4",
    bnb_4bit_compute_dtype=getattr(torch, "float16"),
    bnb_4bit_use_double_quant=False,
)
if model_path is not None:
    based_model = AutoModelForCausalLM.from_pretrained(model_path,
    quantization_config=quant_config if args.quant else None,
    # torch_dtype=torch.float16,
    torch_dtype= compute_dtype if torch.cuda.is_bf16_supported() else torch.float16 if '0.5B' not in args.model_name else torch.float32,
    device_map={'': torch.cuda.current_device()},
    attn_implementation=attn_implementation
    )
    # Nếu checkpoint có chứa optimizer state
    # optimizer = AdamW(based_model.parameters(), lr=2e-4)
    # checkpoint = torch.load(f"{checkpoint_path}/optimizer.pt")
    # optimizer.load_state_dict(checkpoint["optimizer_state_dict"])
    # optimizer = optimizer.to("cuda")

    # scheduler = checkpoint.get("scheduler_state_dict", None)
    # if scheduler:
    #     scheduler.load_state_dict(checkpoint["scheduler_state_dict"])
# Nếu bạn dùng learning rate scheduler
else:
    based_model = AutoModelForCausalLM.from_pretrained(model_name,
    quantization_config=quant_config if args.quant else None,
    #   torch_dtype=torch.float32,
    # torch_dtype=torch.float16,
    torch_dtype= compute_dtype if torch.cuda.is_bf16_supported() else torch.float16 if '0.5B' not in args.model_name else torch.float32,
    #   device_map={"":0}
    device_map={'': torch.cuda.current_device()},
    attn_implementation=attn_implementation
    )

tokenizer = AutoTokenizer.from_pretrained(model_name)

peft_params = LoraConfig(
    r= args.rank,
    lora_alpha=16,
    lora_dropout=0.1,
    bias="none",
    task_type="CAUSAL_LM",
    target_modules= 'all-linear'
#     target_modules=[
# "q_proj",
# "k_proj",
# "v_proj",
# "o_proj",
# "gate_proj",
# "up_proj",
# "down_proj",
# "lm_head",
# ] 
)

import numpy as np
# import matplotlib.pyplot as plt
question='problem'
answer='solution'
suffix = "<|im_end|>"
if type == 'llama':
    suffix = "<|eot_id|>"
elif type == 'phi':
        suffix = "<|endoftext|>"
if args.dataset == 'gsm8k':
    dataset = datasets.load_dataset('gsm8k', "main")
    dataset = dataset['train'].train_test_split(test_size=0.001)
    train_dataset = dataset['train']
    test_dataset = dataset['test']
    question = 'question'
    answer = 'answer'
elif args.dataset == 'metamath':
    dataset = datasets.load_dataset(f"Colder203/meta_math_smaller_than_{args.max_length}")
    #choose only type=='gsm_ansaug'
    train_dataset = dataset['train']
    if args.filter is not None:
        train_dataset = train_dataset.filter(lambda x: x['type'].lower() == f'{args.filter}_ansaug')
    
    if args.num_samples is not None:
        train_dataset = train_dataset.select(np.random.choice(len(train_dataset), args.num_samples))
        args.num_samples = len(train_dataset)
    print(train_dataset)
    dataset = train_dataset.train_test_split(test_size=0.001)
    
    train_dataset = dataset['train']
    test_dataset = dataset['test']
    question = 'query'
    answer = 'response'
else:
    dataset = datasets.load_dataset("lighteval/MATH", "all")
    train_dataset, test_dataset = dataset['train'], dataset['test']
    question = 'problem'
    answer = 'solution'
print('load dataset ok')
# def preprocess_function(examples):
    
    
#     # print(targets[0])
#     # model_inputs = tokenizer(inputs, max_length=512, truncation=True, padding="max_length")
#     model_inputs = tokenizer(inputs,
#                             #  max_length=args.max_length, padding = args.padding,
#                             #  max_length=512, 
#                             # padding = 'longest',
#                             # return_tensors="pt"
#                             #  truncation='longest'
#                              )
#     # labels = tokenizer(targets, max_length=512, truncation=True, padding = True)
#     labels = tokenizer(targets,
#                         #   max_length=args.max_length, padding = args.padding,
#                     #    max_length=512, 
#                     # padding = 'longest',
#                     # return_tensors="pt"
#                     #    truncation=True
#                        )
#     model_inputs['input_ids'], labels['input_ids'] = model_inputs['input_ids'] + labels['input_ids'], [-100] * len(model_inputs['input_ids']) + labels['input_ids']
#     print(len(model_inputs['input_ids']))
#     print(len(labels['input_ids']))
#     model_inputs["labels"] = labels["input_ids"]
    
    # return model_inputs
def _tokenize_fn(strings: Sequence[str], tokenizer: transformers.PreTrainedTokenizer) -> Dict:
    """Tokenize a list of strings."""
    tokenized_list = [
        tokenizer(
            text,
            return_tensors="pt",
            padding="longest",
            max_length=tokenizer.model_max_length,
            truncation=False,
        )
        for text in strings
    ]
    input_ids = labels = [tokenized.input_ids[0] for tokenized in tokenized_list]
    input_ids_lens = labels_lens = [
        tokenized.input_ids.ne(tokenizer.pad_token_id).sum().item() for tokenized in tokenized_list
    ]
    return dict(
        input_ids=input_ids,
        labels=labels,
        input_ids_lens=input_ids_lens,
        labels_lens=labels_lens,
    )


def preprocess(
    sources: Sequence[str],
    targets: Sequence[str],
    tokenizer: transformers.PreTrainedTokenizer,
) -> Dict:
    """Preprocess the data by tokenizing."""
    examples = [s + t for s, t in zip(sources, targets)]
    examples_tokenized, sources_tokenized = [_tokenize_fn(strings, tokenizer) for strings in (examples, sources)]
    input_ids = examples_tokenized["input_ids"]
    labels = copy.deepcopy(input_ids)
    for label, source_len in zip(labels, sources_tokenized["input_ids_lens"]):
        label[:source_len] = IGNORE_INDEX
    return dict(input_ids=input_ids, labels=labels)

def train_tokenize_function(examples, tokenizer):
    if type == 'qwen':
        prompt = prompt_qwen
    elif type == 'phi':
        prompt = prompt_phi
            
    elif type == 'llama':
        prompt = prompt_llama


    sources = [prompt.format(instruction = quest) for quest in examples[question]]
    # print(inputs[0])
    
    targets = [f"{completion}" + suffix for completion in examples[answer]]
    

    data_dict = preprocess(sources, targets, tokenizer)
    return data_dict

IGNORE_INDEX = -100
@dataclass
class DataCollatorForSupervisedDataset(object):
    """Collate examples for supervised fine-tuning."""

    tokenizer: transformers.PreTrainedTokenizer

    def __call__(self, instances: Sequence[Dict]) -> Dict[str, torch.Tensor]:
        input_ids, labels = tuple([instance[key] for instance in instances] for key in ("input_ids", "labels"))
        input_ids = [torch.tensor(x) for x in input_ids]
        input_ids = torch.nn.utils.rnn.pad_sequence(
            input_ids, batch_first=True, padding_value=self.tokenizer.pad_token_id
        )
        labels = [torch.tensor(x) for x in labels]
        labels = torch.nn.utils.rnn.pad_sequence(labels, batch_first=True, padding_value=IGNORE_INDEX)
        return dict(
            input_ids=input_ids,
            labels=labels,
            attention_mask=input_ids.ne(self.tokenizer.pad_token_id),
        )

tokenizer.truncation_side = "left"
tokenizer.padding_side = "left"
tokenizer.pad_token = tokenizer.eos_token
data_collator = DataCollatorForSupervisedDataset(tokenizer=tokenizer)
# print(tokenizer('|finetune_right_pad_id|>'))

# tokenized_dataset = train_dataset.map(preprocess_function, batched=2, remove_columns=train_dataset.column_names)
tokenized_dataset = train_dataset.map(
    train_tokenize_function,
    batched = True,
    batch_size = 3000,
    remove_columns=train_dataset.column_names,
    fn_kwargs={"tokenizer": tokenizer}
)
print(tokenized_dataset)
tokenized_eval_dataset = test_dataset.map(
    train_tokenize_function,
    batched = True,
    batch_size = 3000,
    remove_columns=test_dataset.column_names,
    fn_kwargs={"tokenizer": tokenizer}
)

# %cd /content/drive/MyDrive/qwen

training_params = TrainingArguments(
    output_dir=f"{type}/results" if args.save_path is None else args.save_path,
    num_train_epochs=5,
    per_device_train_batch_size=args.batch_size,
    gradient_accumulation_steps=1,
    logging_steps=200,
    learning_rate=2e-4,
    logging_dir=f"{type}/logs",
    save_strategy="epoch" if args.save_strategy == 'epoch' else "steps",
    save_steps=6000 if args.save_steps is None else args.save_steps,
    # fp32=True,
    bf16=torch.cuda.is_bf16_supported() and '0.5B' not in args.model_name,
    fp16 = (not torch.cuda.is_bf16_supported()) and '0.5B' not in args.model_name,
    # tf32 = torch.backends.cuda.matmul.allow_tf32,
    # evaluation_strategy="epoch",
    report_to = "tensorboard",
    push_to_hub=args.push_to_hub,
    hub_model_id = f"{type}_{args.dataset}_{args.num_samples}_{args.rank}_{args.attn}_{args.quant}" if args.push_to_hub else None,
    # hub_model_id = 'Colder203/qwen0.5b_gsm8k', #uncommnt this
    hub_token = HUGGINGFACE_TOKEN
)
print("is support bf16: ", training_params.bf16)
# print(training_params.tf32)
trainer = SFTTrainer(
    model=based_model,
    train_dataset=tokenized_dataset,
    # eval_dataset=tokenized_eval_dataset,
    peft_config=peft_params if '0.5B' not in args.model_name else None, #full finetuning if model is 0.5B
    # max_seq_length=2048,
    args=training_params,
    packing=False,
    tokenizer = tokenizer,
    data_collator = data_collator
    # optimizers=(optimizer, scheduler) if model_path is not None else None,
    # overlap_comm = False ,
    
)

#check gpu memory
gpu_stats = torch.cuda.get_device_properties(0)
start_gpu_memory = round(torch.cuda.max_memory_reserved() / 1024 / 1024 / 1024, 3)
max_memory = round(gpu_stats.total_memory / 1024 / 1024 / 1024, 3)
print(f"GPU = {gpu_stats.name}. Max memory = {max_memory} GB.")
print(f"{start_gpu_memory} GB of memory reserved.")


if args.model_path is not None:
    trainer.train(resume_from_checkpoint=args.model_path)
else:
    trainer.train()

trainer.save_model(model_name)