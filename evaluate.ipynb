{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/intern/miniconda3/envs/hoang/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, Trainer, TrainingArguments, Trainer, pipeline\n",
    "from peft import LoraConfig\n",
    "from datasets import Dataset\n",
    "import datasets\n",
    "from trl import SFTTrainer, PPOTrainer\n",
    "\n",
    "from tqdm import tqdm\n",
    "#load model name\n",
    "# model_name = \"qwen/Qwen2.5-0.5B\"\n",
    "import argparse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = 'qwen/Qwen2.5-1.5B'\n",
    "model_pretrained_path = 'cp1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "based_model = AutoModelForCausalLM.from_pretrained(model_name,\n",
    "                                                   low_cpu_mem_usage=True,\n",
    "    # quantization_config=quant_config,\n",
    "    # torch_dtype=torch.float16,\n",
    "    device_map={\"\":0}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Qwen2ForCausalLM(\n",
       "  (model): Qwen2Model(\n",
       "    (embed_tokens): Embedding(151936, 1536)\n",
       "    (layers): ModuleList(\n",
       "      (0-27): 28 x Qwen2DecoderLayer(\n",
       "        (self_attn): Qwen2SdpaAttention(\n",
       "          (q_proj): lora.Linear(\n",
       "            (base_layer): Linear(in_features=1536, out_features=1536, bias=True)\n",
       "            (lora_dropout): ModuleDict(\n",
       "              (default): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (lora_A): ModuleDict(\n",
       "              (default): Linear(in_features=1536, out_features=128, bias=False)\n",
       "            )\n",
       "            (lora_B): ModuleDict(\n",
       "              (default): Linear(in_features=128, out_features=1536, bias=False)\n",
       "            )\n",
       "            (lora_embedding_A): ParameterDict()\n",
       "            (lora_embedding_B): ParameterDict()\n",
       "            (lora_magnitude_vector): ModuleDict()\n",
       "          )\n",
       "          (k_proj): Linear(in_features=1536, out_features=256, bias=True)\n",
       "          (v_proj): lora.Linear(\n",
       "            (base_layer): Linear(in_features=1536, out_features=256, bias=True)\n",
       "            (lora_dropout): ModuleDict(\n",
       "              (default): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (lora_A): ModuleDict(\n",
       "              (default): Linear(in_features=1536, out_features=128, bias=False)\n",
       "            )\n",
       "            (lora_B): ModuleDict(\n",
       "              (default): Linear(in_features=128, out_features=256, bias=False)\n",
       "            )\n",
       "            (lora_embedding_A): ParameterDict()\n",
       "            (lora_embedding_B): ParameterDict()\n",
       "            (lora_magnitude_vector): ModuleDict()\n",
       "          )\n",
       "          (o_proj): Linear(in_features=1536, out_features=1536, bias=False)\n",
       "          (rotary_emb): Qwen2RotaryEmbedding()\n",
       "        )\n",
       "        (mlp): Qwen2MLP(\n",
       "          (gate_proj): Linear(in_features=1536, out_features=8960, bias=False)\n",
       "          (up_proj): Linear(in_features=1536, out_features=8960, bias=False)\n",
       "          (down_proj): Linear(in_features=8960, out_features=1536, bias=False)\n",
       "          (act_fn): SiLU()\n",
       "        )\n",
       "        (input_layernorm): Qwen2RMSNorm((1536,), eps=1e-06)\n",
       "        (post_attention_layernorm): Qwen2RMSNorm((1536,), eps=1e-06)\n",
       "      )\n",
       "    )\n",
       "    (norm): Qwen2RMSNorm((1536,), eps=1e-06)\n",
       "    (rotary_emb): Qwen2RotaryEmbedding()\n",
       "  )\n",
       "  (lm_head): Linear(in_features=1536, out_features=151936, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "based_model.from_pretrained(model_pretrained_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# based_model = AutoModelForCausalLM.from_pretrained(model_pretrained_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|im_start|>user\n",
      "A merchant wants to make a choice of purchase between 2 purchase plans: jewelry worth $5,000 or electronic gadgets worth $8,000. His financial advisor speculates that the jewelry market will go up 2.5% while the electronic gadgets market will rise 1.2% within the same month. If the merchant is looking to maximize profit at the end of this month by making a choice, how much profit would this be?<|im_end|>\n",
      "\n",
      "<|im_start|>assistant\n"
     ]
    }
   ],
   "source": [
    "question = \"A merchant wants to make a choice of purchase between 2 purchase plans: jewelry worth $5,000 or electronic gadgets worth $8,000. His financial advisor speculates that the jewelry market will go up 2.5% while the electronic gadgets market will rise 1.2% within the same month. If the merchant is looking to maximize profit at the end of this month by making a choice, how much profit would this be?\"\n",
    "prompt1 = '''<|im_start|>user\n",
    "{question}<|im_end|>\n",
    "\n",
    "<|im_start|>assistant'''\n",
    "\n",
    "input1 = prompt1.format(question = question)\n",
    "print(input1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# hard test 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|im_start|>user\n",
      "John takes care of 10 dogs. Each dog takes .5 hours a day to walk and take care of their business. How many hours a week does he spend taking care of dogs?<|im_end|>\n",
      "\n",
      "<|im_start|>assistant\n"
     ]
    }
   ],
   "source": [
    "question = \"John takes care of 10 dogs. Each dog takes .5 hours a day to walk and take care of their business. How many hours a week does he spend taking care of dogs?\"\n",
    "prompt2 = '''<|im_start|>user\n",
    "{question}<|im_end|>\n",
    "\n",
    "<|im_start|>assistant'''\n",
    "\n",
    "input2 = prompt2.format(question = question)\n",
    "print(input2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run part"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|im_start|>user\n",
      "A merchant wants to make a choice of purchase between 2 purchase plans: jewelry worth $5,000 or electronic gadgets worth $8,000. His financial advisor speculates that the jewelry market will go up 2.5% while the electronic gadgets market will rise 1.2% within the same month. If the merchant is looking to maximize profit at the end of this month by making a choice, how much profit would this be?<|im_end|>\n",
      "\n",
      "<|im_start|>assistant\n",
      "The profit from the jewelry purchase would be $5,000 * 2.5% = $125. The profit from the electronic gadgets purchase would be $8,000 * 1.2% = $96. The total profit would be $125 + $96 = $221.\n",
      "\n",
      "imdi\n",
      "imdiassistant\n",
      "The profit from the jewelry purchase would be $5,000 * 2.5% = $125. The profit from the electronic gadgets purchase would be $8,000 * 1.2% = $96. The total profit would be $125 + $96 = $221.\n",
      "\n",
      "imdi\n",
      "imdiassistant\n",
      "The profit from the jewelry purchase would be $5,000 * 2.5% = $125. The profit from the electronic gadgets purchase would be $8,000 * 1.2% = $96. The total profit would be $125 + $96 = $221.\n",
      "\n",
      "imdi\n",
      "imdiassistant\n",
      "The profit from the jewelry purchase would be $5,000 * 2.5% = $125. The profit from the electronic gadgets purchase would be $8,000 * 1.2% = $96. The total profit would be $125 + $96 = $221.\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "output = based_model.generate(tokenizer(input1, return_tensors='pt').input_ids.to('cuda'), max_new_tokens = 300, eos_token_id = [151643, 151645])\n",
    "print(tokenizer.decode(output[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|im_start|>user\n",
      "John takes care of 10 dogs. Each dog takes .5 hours a day to walk and take care of their business. How many hours a week does he spend taking care of dogs?<|im_end|>\n",
      "\n",
      "<|im_start|>assistant\n",
      "John takes care of 10 dogs. Each dog takes .5 hours a day to walk and take care of their business. How many hours a week does he spend taking care of dogs?界第一行\n",
      "\n",
      "界第一行\n",
      "界第一行\n",
      "界第一行\n",
      "界第一行\n",
      "界第一行\n",
      "界第一行\n",
      "界第一行\n",
      "界第一行\n",
      "界第一行\n",
      "界第一行\n",
      "界第一行\n",
      "界第一行\n",
      "界第一行\n",
      "界第一行\n",
      "界第一行\n",
      "界第一行\n",
      "界第一行\n",
      "界第一行\n",
      "界第一行\n",
      "界第一行\n",
      "界第一行\n",
      "界第一行\n",
      "界第一行\n",
      "界第一行\n",
      "界第一行\n",
      "界第一行\n",
      "界第一行\n",
      "界第一行\n",
      "界第一行\n",
      "界第一行\n",
      "界第一行\n",
      "界第一行\n",
      "界第一行\n",
      "界第一行\n",
      "界第一行\n",
      "界第一行\n",
      "界第一行\n",
      "界第一行\n",
      "界第一行\n",
      "界第一行\n",
      "界第一行\n",
      "界第一行\n",
      "界第一行\n",
      "界第一行\n",
      "界第一行\n",
      "界第一行\n",
      "界第一行\n",
      "界第一行\n",
      "界第一行\n",
      "界第一行\n",
      "界第一行\n",
      "界第一行\n",
      "界第一行\n",
      "界第一行\n",
      "界第一行\n",
      "界第一行\n",
      "界第一行\n",
      "界第一行\n",
      "界第一行\n",
      "界第一行\n",
      "界第一行\n",
      "界第一行\n",
      "界第一行\n",
      "界第一行\n",
      "界第一行\n",
      "界第一行\n",
      "界第一行\n",
      "界第一行\n",
      "界第一行\n",
      "界第一行\n",
      "界第一行\n",
      "界第一行\n",
      "界第一行\n",
      "界第一行\n",
      "界第一行\n",
      "界第一行\n",
      "界第一行\n",
      "界第一行\n",
      "界第一行\n",
      "界第一行\n",
      "界第一行\n",
      "界第一行\n",
      "界第一行\n",
      "界第一行\n",
      "界第一行\n",
      "界第一行\n"
     ]
    }
   ],
   "source": [
    "output = based_model.generate(tokenizer(input2, return_tensors='pt').input_ids.to('cuda'), max_new_tokens = 300, eos_token_id = [151643, 151645])\n",
    "print(tokenizer.decode(output[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hoang",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
